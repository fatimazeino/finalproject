# -*- coding: utf-8 -*-
"""Bostonhouseprices.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OrwoaGwoiGM4rC67_RJI6McC2TzSdaLW
"""

# This program foresee Boston House Prices
# Author: Fatima Zeino

# Commented out IPython magic to ensure Python compatibility.
#import dependencies
import numpy as np
import matplotlib.pyplot as plt 

import pandas as pd
import seaborn as sns 

# %matplotlib inline

from sklearn import linear_model
from sklearn.model_selection import train_test_split

#Load the Boston Housing Data Set from sklearn.datasets and print it
from sklearn.datasets import load_boston
BostonDataset = load_boston()
print(BostonDataset)

#print the value of the BostonDataset
print(BostonDataset.keys())

#the description of all the features
print(BostonDataset.DESCR)

#Transform the data set into a data frame
BostonDataframe = pd.DataFrame(BostonDataset.data, columns=BostonDataset.feature_names)
BostonDataframe.head()

#create a new column of target values and add it to the dataframe
BostonDataframe['MEDV'] = BostonDataset.target

#preproccessing the dataset
BostonDataframe.isnull().sum()

#plot target variable MEDV
BostonDataframe['MEDV'].plot(kind= 'hist')
plt.show()

#plot the distribution of the target variable MEDV
sns.set(rc={'figure.figsize':(11.7,8.27)})
sns.distplot(BostonDataframe['MEDV'], bins=30)
plt.show()

#create a correlation matrix
CorrelationMatrix = BostonDataframe.corr().round(2)
# annot = True to print the values inside the square
sns.heatmap(data=CorrelationMatrix, annot=True)

#scatter plot

plt.figure(figsize=(20, 5))

#RM has a strong positive correlation with MEDV (0.7) where as LSTAT has a high negative correlation with MEDV(-0.74)
FeatureVariables = ['LSTAT', 'RM']
TargetValue = BostonDataframe['MEDV']

for i, col in enumerate(FeatureVariables):
    plt.subplot(1, len(FeatureVariables) , i+1)
    x = BostonDataframe[col]
    y = TargetValue
    plt.scatter(x, y, marker='o')
    plt.title(col)
    plt.xlabel(col)
    plt.ylabel('MEDV')

X = pd.DataFrame(np.c_[BostonDataframe['LSTAT'], BostonDataframe['RM']], columns = ['LSTAT','RM'])
Y = BostonDataframe['MEDV']

#Get some statistics from our data set, count, mean standard deviation etc.
X.describe()

#Initialize the linear regression model
reg = linear_model.LinearRegression()

XTrain, XTest, YTrain, YTest = train_test_split(X, Y, test_size = 0.2, random_state=5)
print(XTrain.shape)
print(XTest.shape)
print(YTrain.shape)
print(YTest.shape)

#Train our model with the training data
reg.fit(XTrain, YTrain)
#Print the coefecients/weights for each feature/column of our model
print(reg.coef_)

#print our price predictions on our test data
YPred = reg.predict(XTest)
print(YPred)

#print the actual values
print(YTest)

#check the model performance / accuracy using mean squared error (MSE)
print(np.mean((YPred-YTest)**2))

from sklearn.metrics import mean_squared_error
#check the model performance / accuracy using mean squared error (MSE) & sklearn.metrics 
print(mean_squared_error(YTest, YPred))

#check the model performance / accuracy using r2_score
from sklearn.metrics import r2_score
r2_score(YTest, YPred)

#trying another regressor DecisionTreeRegressor to compare the resutls

from sklearn.tree import DecisionTreeRegressor
regressor2 = DecisionTreeRegressor()
regressor2.fit(XTrain, YTrain)

Y_pred2 = regressor2.predict(XTest)

from sklearn.metrics import r2_score
r2_score(YTest, Y_pred2)

#trying another regressor RandomForestRegressor to compare the resutls

from sklearn.ensemble import RandomForestRegressor
model = RandomForestRegressor()
model.fit(XTrain, YTrain)

Y_pred3 = model.predict(XTest)

from sklearn.metrics import r2_score
r2_score(YTest, Y_pred3)

#trying another regressor RandomForestRegressor with n_estimators=100  to compare the resutls

from sklearn.ensemble import RandomForestRegressor
regressor3 = RandomForestRegressor(n_estimators=100)
regressor3.fit(XTrain, YTrain)


Y_pred4 = regressor3.predict(XTest)

from sklearn.metrics import r2_score
r2_score(YTest, Y_pred4)

#trying another regressor neural network to compare the resutls

n_cols = XTrain.shape[1]
n_cols

import keras
from keras.layers import Dense
from keras.models import Sequential


model = Sequential()
model.add(Dense(100, activation='relu', input_shape=(n_cols,)))
# Add the second layer
model.add(Dense(100, activation='relu'))
# Add the output layer
model.add(Dense(1))

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])
   
# Define early_stopping_monitor
from keras.callbacks import EarlyStopping

early_stopping_monitor = EarlyStopping(patience=2)

history=model.fit(XTrain, YTrain, validation_split=0.3, epochs=30, callbacks= [early_stopping_monitor])

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1,len(loss)+1)

plt.plot(epochs,loss,'bo',label='Training loss')
plt.plot(epochs,val_loss,'b',label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

predictions = model.predict(XTest)
print(predictions)

from sklearn.metrics import r2_score
r2_score(YTest, predictions)